{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import networkx as nx \n",
    "import pandas as pd \n",
    "import os \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, torch_geometric\n",
    "from torch_geometric.data import Dataset, DenseDataLoader, Data\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import DenseSAGEConv, dense_diff_pool\n",
    "import os.path as osp\n",
    "from math import ceil\n",
    "from Variables import *\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import io\n",
    "import string\n",
    "from flask import Flask, jsonify, request, render_template\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Variables(edge_file):\n",
    "\n",
    "    node250_df = pd.read_csv(r\"C:\\Users\\Vishnu\\Documents\\ijhdfbgjws\\node\\freesurfer_thickness_fsaverage_smoothing10_size250_edgeweight_manhattan_graynet-nodes1.csv\")\n",
    "    node250 = node250_df.iloc[:, 2:]\n",
    "    dat250 = node250_df.set_index('~id').to_dict('index').items()\n",
    "\n",
    "    x250 = glob.glob(edge_file)\n",
    "\n",
    "    y250 = np.zeros(shape=(118, 1))\n",
    "    nf250_array = []\n",
    "\n",
    "    for count in range(118):\n",
    "        nf250_array.append(node250)\n",
    "    \n",
    "    y_df = pd.read_csv(r\"C:\\Users\\Vishnu\\Documents\\y_label.csv\")\n",
    "    y = y_df.to_numpy()\n",
    "\n",
    "    p250 = []\n",
    "    k250 = []\n",
    "\n",
    "    for count in range(len(x250)):\n",
    "        f = pd.read_csv(x250[count], error_bad_lines=False, encoding='latin1')\n",
    "        f.fillna(0.0001)\n",
    "\n",
    "        if len(f) == 674541:\n",
    "            G = nx.from_pandas_edgelist(f, '~from', '~to', edge_attr=True)\n",
    "            G.add_nodes_from(dat250)\n",
    "            p250.append(nx.convert_matrix.to_numpy_array(G, weight = 'd3:double'))\n",
    "            k250.append(x250[count])\n",
    "            \n",
    "    adj = (np.array(p250))\n",
    "    X = (np.array(nf250_array))\n",
    "    edge_index = pd.read_excel( r\"C:\\Users\\Vishnu\\Documents\\COOsparse1.xlsx\", engine='openpyxl')\n",
    "    \n",
    "    \n",
    "    return adj, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Variables_run(edge_file):\n",
    "\n",
    "    node250_df = pd.read_csv(r\"C:\\Users\\Vishnu\\Documents\\ijhdfbgjws\\node\\freesurfer_thickness_fsaverage_smoothing10_size250_edgeweight_manhattan_graynet-nodes1.csv\")\n",
    "    node250 = node250_df.iloc[:, 2:]\n",
    "    node250 = node250.to_numpy()\n",
    "    dat250 = node250_df.set_index('~id').to_dict('index').items()\n",
    "\n",
    "    x250 = glob.glob(edge_file)\n",
    "\n",
    "\n",
    "\n",
    "    p250 = []\n",
    "    k250 = []\n",
    "\n",
    "    for count in range(len(x250)):\n",
    "        f = pd.read_csv(x250[count], error_bad_lines=False, encoding='latin1')\n",
    "        f.fillna(0.0001)\n",
    "\n",
    "        if len(f) == 674541:\n",
    "            G = nx.from_pandas_edgelist(f, '~from', '~to', edge_attr=True)\n",
    "            G.add_nodes_from(dat250)\n",
    "            p250.append(nx.convert_matrix.to_numpy_array(G, weight = 'd3:double'))\n",
    "            k250.append(x250[count])\n",
    "            \n",
    "    \n",
    "    adj = (np.array(p250))\n",
    "    edge_index = pd.read_excel( r\"C:\\Users\\Vishnu\\Documents\\COOsparse1.xlsx\", engine='openpyxl')\n",
    "    \n",
    "    X = torch.tensor(node250, dtype=torch.float32)\n",
    "    adj = torch.tensor(adj, dtype=torch.float32)\n",
    "\n",
    "    return adj, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, X, y = Variables(r\"C:\\Users\\Vishnu\\Documents\\Edges\\250_nodal\\*.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 1)\n",
      "(118, 1162, 3)\n",
      "(118, 1162, 1162)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(y))\n",
    "print(np.shape(X))\n",
    "print(np.shape(adj))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n"
     ]
    }
   ],
   "source": [
    "class Alzheimers(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, adj, y):\n",
    "        \n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.adj = torch.tensor(adj, dtype=torch.float32)\n",
    "#         self.edge = edge.type(torch.LongTensor)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return(len(self.y))\n",
    "\n",
    "    def __num_classes__(self):\n",
    "        return len('ar')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        x = self.X[idx]\n",
    "        adj = self.adj[idx]\n",
    "#         edge_index = self.edge[idx].type(torch.LongTensor)\n",
    "        y = self.y[idx]\n",
    "        num_features = 3 \n",
    "\n",
    "        return torch_geometric.data.Data(x=x, adj=adj, y=y, mask=None)\n",
    "#         return {'x': x, 'adj': adj, 'edge_index': edge_index, 'y':y}\n",
    "    \n",
    "dataset = Alzheimers(X, adj, y)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Alzheimers_run(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, adj):\n",
    "        \n",
    "#         self.y = torch.tensor(y, dtype=torch.long)\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.adj = torch.tensor(adj, dtype=torch.float32)\n",
    "#         self.edge = edge.type(torch.LongTensor)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "         return(len(self.X))\n",
    "\n",
    "    def __num_classes__(self):\n",
    "        return len('ar')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        x = self.X[idx]\n",
    "        adj = self.adj[idx]\n",
    "#         edge_index = self.edge[idx].type(torch.LongTensor)\n",
    "#         y = self.y[idx]\n",
    "        num_features = 3 \n",
    "\n",
    "        return torch_geometric.data.Data(x=x, adj=adj, mask=None)\n",
    "#         return {'x': x, 'adj': adj, 'edge_index': edge_index, 'y':y}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118 83 35\n"
     ]
    }
   ],
   "source": [
    "num_instances = len(dataset)\n",
    "test_ratio = 0.3\n",
    "test_size = int(num_instances * test_ratio)\n",
    "train_size = num_instances - test_size\n",
    "\n",
    "print(num_instances, train_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 35\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = torch.utils.data.random_split(dataset, (train_size, test_size))\n",
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "params = {'batch_size': 1,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "max_epochs = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DenseDataLoader(train_data, **params)\n",
    "test_loader = DenseDataLoader(test_data, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels,\n",
    "                 normalize=False, lin=True):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.conv1 = DenseSAGEConv(in_channels, hidden_channels, normalize)\n",
    "#         self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = DenseSAGEConv(hidden_channels, hidden_channels, normalize)\n",
    "#         self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = DenseSAGEConv(hidden_channels, out_channels, normalize)\n",
    "#         self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        if lin is True:\n",
    "            self.lin = torch.nn.Linear(2 * hidden_channels + out_channels,\n",
    "                                       out_channels)\n",
    "        else:\n",
    "            self.lin = None\n",
    "\n",
    "\n",
    "    def forward(self, x, adj, mask=None):\n",
    "#         batch_size, num_nodes, in_channels = x.size()\n",
    "\n",
    "        x0 = x\n",
    "        x1 = F.relu(self.conv1(x0, adj, mask))\n",
    "        x2 = F.relu(self.conv2(x1, adj, mask))\n",
    "        x3 = F.relu(self.conv3(x2, adj, mask))\n",
    "\n",
    "        x = torch.cat([x1, x2, x3], dim=-1)\n",
    "\n",
    "        if self.lin is not None:\n",
    "            x = F.relu(self.lin(x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        max_nodes = 1162\n",
    "        \n",
    "        num_nodes = ceil(0.25 * max_nodes)\n",
    "        self.gnn1_pool = GNN(3, 64, num_nodes)\n",
    "        self.gnn1_embed = GNN(3, 64, 64, lin=False)\n",
    "        \n",
    "        \n",
    "        num_nodes = ceil(0.25 * num_nodes)\n",
    "        self.gnn2_pool = GNN(3 * 64, 64, num_nodes)\n",
    "        self.gnn2_embed = GNN(3 * 64, 64, 64, lin=False)\n",
    "\n",
    "        \n",
    "        num_nodes = ceil(0.25 * num_nodes)\n",
    "        self.gnn3_pool = GNN(3 * 64, 64, num_nodes)\n",
    "        self.gnn3_embed = GNN(3 * 64, 64, 64, lin=False)\n",
    "\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(3 * 64, 64)\n",
    "        self.lin2 = torch.nn.Linear(64, 64)\n",
    "        self.lin3 = torch.nn.Linear(64, 6)\n",
    "\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        s = self.gnn1_pool(x, adj, mask=None)\n",
    "        x = self.gnn1_embed(x, adj, mask=None)\n",
    "\n",
    "        x, adj, l1, e1 = dense_diff_pool(x, adj, s, mask=None)\n",
    "\n",
    "        s = self.gnn2_pool(x, adj)\n",
    "        x = self.gnn2_embed(x, adj)\n",
    "\n",
    "        x, adj, l2, e2 = dense_diff_pool(x, adj, s)\n",
    "\n",
    "        s = self.gnn3_pool(x, adj)\n",
    "        x = self.gnn3_embed(x, adj)\n",
    "        \n",
    "        x, adj, l3, e3 = dense_diff_pool(x, adj, s)\n",
    "\n",
    "        x = x.mean(dim=1)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1), l1 + l2 + l3, e1 + e2 + e3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "criterion = torch.nn.BCELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, _, _ = model(data.x, data.adj)\n",
    "        loss = F.nll_loss(output, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        loss_all += data.y.size(0) * loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        nb_classes = 2\n",
    "\n",
    "\n",
    "        \n",
    "        pred = model(data.x, data.adj)[0].max(dim=1)[1]\n",
    "        correct += pred.eq(data.y.view(-1)).sum().item()\n",
    "        \n",
    "    return correct / len(train_loader), loss_all / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data.x, data.adj)[0].max(dim=1)[1]\n",
    "        correct += pred.eq(data.y.view(-1)).sum().item()\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 800.2579, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 002, Train Loss: 235.8079, Train Acc: 0.0482, Test Acc: 0.1143\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-c5114fe0e647>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mlist2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#     val_acc = test(val_loader)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mlist3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Trail_duplicate\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-569087e4b9b5>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(loader)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Trail_duplicate\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-66-83eabe62cfcd>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, adj, mask)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgnn1_embed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdense_diff_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgnn2_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Trail_duplicate\\lib\\site-packages\\torch_geometric\\nn\\dense\\diff_pool.py\u001b[0m in \u001b[0;36mdense_diff_pool\u001b[1;34m(x, adj, s, mask)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list1 = []\n",
    "list2 = []\n",
    "list3 = []\n",
    "\n",
    "best_val_acc = test_acc = 0\n",
    "for epoch in range(1, 3001):\n",
    "    train_acc, train_loss = train(epoch)\n",
    "    list1.append(train_acc)\n",
    "    list2.append(train_loss)\n",
    "#     val_acc = test(val_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    list3.append(test_acc)\n",
    "        \n",
    "    \n",
    "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, '\n",
    "          f'Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./fix_vishnunet17.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 2\n",
    "predlist=torch.zeros(0,dtype=torch.long)\n",
    "lbllist=torch.zeros(0,dtype=torch.long)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, _, _ = model(data.x, data.adj)\n",
    "        _, preds = torch.max(output, 1)\n",
    "\n",
    "        # Append batch prediction results\n",
    "        predlist=torch.cat([predlist,preds.view(-1)])\n",
    "        lbllist=torch.cat([lbllist,data.y.view(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-e59cf5fa31f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlbllist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mstuff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(predlist.numpy(), lbllist.numpy())\n",
    "\n",
    "stuff = sns.heatmap(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}